# 本地 Markdown 知识库：完整存储 vs 语义切片（Chunking）

针对“在 Mac 本地向量库中直接完整保存 Markdown 文档而不进行切片”的想法，基于第一性原理，我们需要从**数学检索精度**和**模型推理效率**两个维度进行评估。

### 1. 为什么“不切片”会导致搜索失效？

向量化的本质是**“语义压缩”**。

*   **语义稀释 (Semantic Dilution)**：主流 Embedding 模型（如 OpenAI 或 BGE）通常有输入上限（如 512 到 8192 Tokens）。如果一个 Markdown 包含 2 万字，模型会被迫截断或将海量信息压缩成一个固定维度的向量。这导致向量变得极其“平庸”，它能表达“这篇文档是关于 Python 的”，但无法表达“第 345 行关于异步锁的具体逻辑”。
*   **低信噪比**：当你搜索具体问题（如“useEffect 闭包陷阱”）时，一个包含几千行代码的完整文档向量，其匹配度往往远低于一个专门讨论该主题的 500 字切片。

### 2. 对 LLM 推理的影响

*   **注意力涣散**：即使 LLM 上下文窗口极大（如 Claude 3.5 的 200k），如果一次性塞入 5 个各 2 万字的完整文档，模型容易产生 "Lost in the Middle" 现象，难以在无关信息中定位关键代码。
*   **成本与速度**：处理 10 万字的 Token 消耗和等待时间是处理几千字切片的几十倍。

### 3. 最佳实践：父子检索（Parent-Child Retriever）

如果你希望 **“用小切片精准找到位置，但给 LLM 提供整个文档”**，这套策略是最佳折中方案：

1.  **存储阶段**：
    *   将文档切成小块（如 500-1000 字）。
    *   每个切片在数据库中关联其所属的 `document_id` 或文件路径。
2.  **检索阶段**：
    *   使用用户问题检索**切片向量**（确保定位精准）。
    *   命中 Top-K 切片后，程序根据元数据中的路径，回表读取**整篇 Markdown 源码**。
3.  **最终输入**：
    *   将检索到的 2-3 篇**完整文档**喂给 LLM。

**优点**：兼顾了搜索的“快/准”和生成的“全”。

### 4. 什么时候可以“不切片”？

只有满足以下条件，才建议直接存储完整文档：
*   **短文档**：每个 .md 文件都在 2000 字（约 3000 Tokens）以内。
*   **宏观检索**：你的搜索需求仅限于“找哪一本书/哪篇文章提到过某技术”，而非“解决具体的 Bug”。

### 总结建议

*   **不要尝试把长文档强制塞进一个向量**。
*   推荐使用 **LanceDB** 存储切片向量，并在元数据（Metadata）中保存文件路径。
*   检索后，由系统逻辑决定是仅读取切片，还是读取该路径下的全量文件内容提供给 LLM。
