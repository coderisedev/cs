# PostgreSQL æ•°æ®åº“å¤‡ä»½ç³»ç»Ÿå®ç°æ•™ç¨‹

æœ¬æ•™ç¨‹ä»‹ç»å¦‚ä½•ä¸º GCE ç”Ÿäº§ç¯å¢ƒé…ç½®è‡ªåŠ¨åŒ–æ•°æ®åº“å¤‡ä»½ç³»ç»Ÿï¼ŒåŒ…æ‹¬æœ¬åœ°å¤‡ä»½ã€äº‘å­˜å‚¨åŒæ­¥å’Œå¥åº·ç›‘æ§ã€‚

## ç›®å½•

- [æ¶æ„æ¦‚è§ˆ](#æ¶æ„æ¦‚è§ˆ)
- [å‰ç½®æ¡ä»¶](#å‰ç½®æ¡ä»¶)
- [å®ç°æ­¥éª¤](#å®ç°æ­¥éª¤)
  - [1. åˆ›å»ºç›®å½•ç»“æ„](#1-åˆ›å»ºç›®å½•ç»“æ„)
  - [2. åˆ›å»ºå¤‡ä»½è„šæœ¬](#2-åˆ›å»ºå¤‡ä»½è„šæœ¬)
  - [3. é…ç½® rclone è¿æ¥ R2](#3-é…ç½®-rclone-è¿æ¥-r2)
  - [4. é…ç½® PostgreSQL è®¤è¯](#4-é…ç½®-postgresql-è®¤è¯)
  - [5. è®¾ç½® Cron å®šæ—¶ä»»åŠ¡](#5-è®¾ç½®-cron-å®šæ—¶ä»»åŠ¡)
- [ä½¿ç”¨æŒ‡å—](#ä½¿ç”¨æŒ‡å—)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

## æ¶æ„æ¦‚è§ˆ

```mermaid
flowchart TB
    subgraph GCE VM
        PG[(PostgreSQL)]
        CRON[Cron Scheduler]
        SCRIPTS[Backup Scripts]
        LOCAL[/backup/postgres/]
    end

    subgraph "Cloudflare R2"
        R2[cs-backups bucket]
    end

    CRON -->|00:00 daily| SCRIPTS
    CRON -->|02:00 Sunday| SCRIPTS
    CRON -->|every 6h| SCRIPTS

    SCRIPTS -->|pg_dump| PG
    SCRIPTS -->|save| LOCAL
    SCRIPTS -->|rclone sync| R2

    style R2 fill:#f9a825
    style LOCAL fill:#4caf50
```

### å¤‡ä»½ç­–ç•¥

| ç±»å‹ | é¢‘ç‡ | ä¿ç•™æœŸ | å­˜å‚¨ä½ç½® |
|------|------|--------|----------|
| é€»è¾‘å¤‡ä»½ (pg_dump) | æ¯æ—¥ 00:00 | 30 å¤© | æœ¬åœ° + R2 |
| ç‰©ç†å¤‡ä»½ (pg_basebackup) | æ¯å‘¨æ—¥ 02:00 | 4 å‘¨ | æœ¬åœ° + R2 |
| å¥åº·æ£€æŸ¥ | æ¯ 6 å°æ—¶ | - | æ—¥å¿— |

## å‰ç½®æ¡ä»¶

- Ubuntu/Debian ç³»ç»Ÿ
- PostgreSQL å·²å®‰è£…å¹¶è¿è¡Œ
- å…·æœ‰ sudo æƒé™
- Cloudflare R2 è´¦æˆ·ï¼ˆæˆ–å…¶ä»– S3 å…¼å®¹å­˜å‚¨ï¼‰

## å®ç°æ­¥éª¤

### 1. åˆ›å»ºç›®å½•ç»“æ„

```bash
# åˆ›å»ºå¤‡ä»½ç›®å½•
sudo mkdir -p /backup/{postgres/{daily,weekly,wal-archive},scripts,logs}

# è®¾ç½®æƒé™ï¼ˆæ›¿æ¢ your_user ä¸ºå®é™…ç”¨æˆ·åï¼‰
sudo chown -R $USER:$USER /backup

# éªŒè¯ç»“æ„
tree /backup
```

é¢„æœŸè¾“å‡ºï¼š

```
/backup
â”œâ”€â”€ logs
â”œâ”€â”€ postgres
â”‚   â”œâ”€â”€ daily
â”‚   â”œâ”€â”€ wal-archive
â”‚   â””â”€â”€ weekly
â””â”€â”€ scripts
```

### 2. åˆ›å»ºå¤‡ä»½è„šæœ¬

#### 2.1 æ¯æ—¥é€»è¾‘å¤‡ä»½è„šæœ¬

åˆ›å»º `/backup/scripts/backup-daily.sh`ï¼š

```bash
#!/bin/bash
#
# Daily PostgreSQL Backup Script
# Performs logical backups of all production databases
#

set -euo pipefail

# =============================================================================
# Configuration - æ ¹æ®å®é™…ç¯å¢ƒä¿®æ”¹
# =============================================================================

BACKUP_DIR="/backup/postgres/daily"
LOG_FILE="/backup/logs/backup-daily.log"
RETENTION_DAYS=30
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

# Database connection
DB_HOST="localhost"
DB_USER="cs"                    # ä¿®æ”¹ä¸ºå®é™…ç”¨æˆ·
DB_PORT="5432"

# Databases to backup
DATABASES=("medusa_production" "strapi_production")  # ä¿®æ”¹ä¸ºå®é™…æ•°æ®åº“

# R2 remote (configured in rclone)
R2_REMOTE="r2:cs-backups/postgres/daily"
R2_ENABLED=true

# =============================================================================
# Functions
# =============================================================================

log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

log_error() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] ERROR: $1" | tee -a "$LOG_FILE" >&2
}

send_alert() {
    local message=$1
    # Discord webhook (configure DISCORD_WEBHOOK_URL in environment)
    if [[ -n "${DISCORD_WEBHOOK_URL:-}" ]]; then
        curl -s -X POST "$DISCORD_WEBHOOK_URL" \
            -H "Content-Type: application/json" \
            -d "{\"content\": \"ğŸš¨ **Backup Alert**: ${message}\"}" \
            > /dev/null 2>&1 || true
    fi
    log_error "$message"
}

backup_database() {
    local db=$1
    local backup_file="${BACKUP_DIR}/${db}_${TIMESTAMP}.dump"

    log "Starting backup of ${db}..."

    # Use custom format for parallel restore support
    if pg_dump \
        -U "$DB_USER" \
        -h "$DB_HOST" \
        -p "$DB_PORT" \
        -d "$db" \
        --format=custom \
        --compress=6 \
        --file="$backup_file" \
        2>> "$LOG_FILE"; then

        # Verify backup integrity
        if pg_restore --list "$backup_file" > /dev/null 2>&1; then
            local size=$(du -h "$backup_file" | cut -f1)
            log "âœ“ ${db} backup successful: ${backup_file} (${size})"

            # Generate checksum
            sha256sum "$backup_file" > "${backup_file}.sha256"
            return 0
        else
            log_error "${db} backup verification failed!"
            rm -f "$backup_file"
            return 1
        fi
    else
        log_error "${db} pg_dump failed!"
        return 1
    fi
}

sync_to_r2() {
    if [[ "$R2_ENABLED" != "true" ]]; then
        log "R2 sync disabled, skipping..."
        return 0
    fi

    if ! command -v rclone &> /dev/null; then
        log "rclone not installed, skipping R2 sync..."
        return 0
    fi

    log "Syncing to R2..."
    if rclone sync "$BACKUP_DIR" "$R2_REMOTE" \
        --transfers=4 \
        --checkers=8 \
        --log-file="$LOG_FILE" \
        --log-level=INFO \
        2>> "$LOG_FILE"; then
        log "âœ“ R2 sync completed"
    else
        log_error "R2 sync failed"
        return 1
    fi
}

cleanup_old_backups() {
    log "Cleaning up backups older than ${RETENTION_DAYS} days..."

    local count=$(find "$BACKUP_DIR" -name "*.dump" -mtime +${RETENTION_DAYS} | wc -l)

    if [[ $count -gt 0 ]]; then
        find "$BACKUP_DIR" -name "*.dump" -mtime +${RETENTION_DAYS} -delete
        find "$BACKUP_DIR" -name "*.sha256" -mtime +${RETENTION_DAYS} -delete
        log "âœ“ Cleaned up ${count} old backup(s)"
    else
        log "No old backups to clean up"
    fi
}

# =============================================================================
# Main
# =============================================================================

main() {
    log "=========================================="
    log "Daily Backup Started"
    log "=========================================="

    mkdir -p "$BACKUP_DIR"

    local failed=0

    for db in "${DATABASES[@]}"; do
        if ! backup_database "$db"; then
            send_alert "Failed to backup ${db}"
            failed=1
        fi
    done

    if ! sync_to_r2; then
        send_alert "Failed to sync backups to R2"
        failed=1
    fi

    cleanup_old_backups

    log "=========================================="
    if [[ $failed -eq 0 ]]; then
        log "Daily Backup Completed Successfully"
    else
        log "Daily Backup Completed with Errors"
    fi
    log "=========================================="

    return $failed
}

main "$@"
```

#### 2.2 æ¯å‘¨ç‰©ç†å¤‡ä»½è„šæœ¬

åˆ›å»º `/backup/scripts/backup-weekly.sh`ï¼š

```bash
#!/bin/bash
#
# Weekly PostgreSQL Physical Backup Script
#

set -euo pipefail

BACKUP_DIR="/backup/postgres/weekly"
LOG_FILE="/backup/logs/backup-weekly.log"
RETENTION_WEEKS=4
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

DB_HOST="localhost"
DB_USER="cs"
DB_PORT="5432"

R2_REMOTE="r2:cs-backups/postgres/weekly"
R2_ENABLED=true

log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

log_error() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] ERROR: $1" | tee -a "$LOG_FILE" >&2
}

perform_basebackup() {
    local backup_path="${BACKUP_DIR}/base_${TIMESTAMP}"
    local backup_archive="${backup_path}.tar.gz"

    log "Starting physical backup..."
    mkdir -p "$backup_path"

    if pg_basebackup \
        -U "$DB_USER" \
        -h "$DB_HOST" \
        -p "$DB_PORT" \
        -D "$backup_path" \
        -Ft -z -Xs -P \
        2>> "$LOG_FILE"; then

        log "Creating archive..."
        cd "$BACKUP_DIR"
        tar -czf "$backup_archive" -C "$BACKUP_DIR" "base_${TIMESTAMP}"
        rm -rf "$backup_path"

        local size=$(du -h "$backup_archive" | cut -f1)
        log "âœ“ Physical backup successful: ${backup_archive} (${size})"
        sha256sum "$backup_archive" > "${backup_archive}.sha256"
        return 0
    else
        log_error "pg_basebackup failed!"
        rm -rf "$backup_path"
        return 1
    fi
}

sync_to_r2() {
    if [[ "$R2_ENABLED" != "true" ]] || ! command -v rclone &> /dev/null; then
        return 0
    fi

    log "Syncing to R2..."
    local latest=$(ls -t "${BACKUP_DIR}"/base_*.tar.gz 2>/dev/null | head -1)

    if [[ -n "$latest" ]]; then
        rclone copy "$latest" "$R2_REMOTE" --log-file="$LOG_FILE" 2>> "$LOG_FILE"
        rclone copy "${latest}.sha256" "$R2_REMOTE" 2>> "$LOG_FILE" || true
        log "âœ“ R2 sync completed"
    fi
}

cleanup_old_backups() {
    local retention_days=$((RETENTION_WEEKS * 7))
    log "Cleaning up backups older than ${RETENTION_WEEKS} weeks..."
    find "$BACKUP_DIR" -name "base_*.tar.gz" -mtime +${retention_days} -delete
    find "$BACKUP_DIR" -name "base_*.tar.gz.sha256" -mtime +${retention_days} -delete
}

main() {
    log "=========================================="
    log "Weekly Physical Backup Started"
    log "=========================================="

    mkdir -p "$BACKUP_DIR"
    perform_basebackup
    sync_to_r2
    cleanup_old_backups

    log "=========================================="
    log "Weekly Backup Completed"
    log "=========================================="
}

main "$@"
```

#### 2.3 æ¢å¤è„šæœ¬

åˆ›å»º `/backup/scripts/restore.sh`ï¼š

```bash
#!/bin/bash
#
# PostgreSQL Restore Script
#

set -euo pipefail

BACKUP_DIR="/backup/postgres/daily"
LOG_FILE="/backup/logs/restore.log"
DB_HOST="localhost"
DB_USER="cs"
DB_PORT="5432"

log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

usage() {
    cat << EOF
PostgreSQL Restore Script

Usage:
    $0 <command> [options]

Commands:
    list [database]               List available backups
    verify <backup_file>          Verify backup integrity
    restore <backup> <target_db>  Restore backup to target database
    restore-table <backup> <target_db> <table>  Restore single table
    download <database> <date>    Download backup from R2

Examples:
    $0 list
    $0 list medusa_production
    $0 verify /backup/postgres/daily/medusa_production_20240115.dump
    $0 restore /backup/postgres/daily/medusa_production_20240115.dump medusa_restore
    $0 restore-table /backup/postgres/daily/medusa_production_20240115.dump medusa_production product

EOF
    exit 1
}

list_backups() {
    local filter=${1:-}
    echo "Available backups in ${BACKUP_DIR}:"
    echo "=============================================="

    if [[ -n "$filter" ]]; then
        ls -lh "${BACKUP_DIR}/${filter}"_*.dump 2>/dev/null | awk '{print $9, $5, $6, $7, $8}' | sort -r | head -20
    else
        ls -lh "${BACKUP_DIR}"/*.dump 2>/dev/null | awk '{print $9, $5, $6, $7, $8}' | sort -r | head -20
    fi

    echo ""
    echo "Total backups: $(ls -1 "${BACKUP_DIR}"/*.dump 2>/dev/null | wc -l)"
}

verify_backup() {
    local backup_file=$1

    [[ ! -f "$backup_file" ]] && { log "Backup file not found: $backup_file"; return 1; }

    log "Verifying backup: $backup_file"

    if ! pg_restore --list "$backup_file" > /dev/null 2>&1; then
        log "ERROR: Backup file is corrupted"
        return 1
    fi

    if [[ -f "${backup_file}.sha256" ]]; then
        if sha256sum -c "${backup_file}.sha256" > /dev/null 2>&1; then
            log "âœ“ Checksum verified"
        else
            log "ERROR: Checksum mismatch!"
            return 1
        fi
    fi

    log "âœ“ Backup verified successfully"
}

restore_database() {
    local backup_file=$1
    local target_db=$2

    [[ ! -f "$backup_file" ]] && { log "Backup file not found"; return 1; }

    log "Restoring to database: $target_db"

    # Check if target exists
    if psql -U "$DB_USER" -h "$DB_HOST" -lqt | cut -d \| -f 1 | grep -qw "$target_db"; then
        read -p "Database exists. Drop and recreate? (y/N): " confirm
        [[ "$confirm" != "y" ]] && return 1
        dropdb -U "$DB_USER" -h "$DB_HOST" "$target_db" || true
    fi

    createdb -U "$DB_USER" -h "$DB_HOST" "$target_db"

    if pg_restore -U "$DB_USER" -h "$DB_HOST" -d "$target_db" --jobs=4 "$backup_file" 2>> "$LOG_FILE"; then
        log "âœ“ Database restored successfully"
    else
        log "Restore completed with warnings (check log)"
    fi
}

restore_table() {
    local backup_file=$1
    local target_db=$2
    local table_name=$3

    log "Restoring table '$table_name' to $target_db"

    psql -U "$DB_USER" -h "$DB_HOST" -d "$target_db" \
        -c "DROP TABLE IF EXISTS \"$table_name\" CASCADE;" 2>/dev/null || true

    pg_restore -U "$DB_USER" -h "$DB_HOST" -d "$target_db" \
        --table="$table_name" "$backup_file" 2>> "$LOG_FILE"

    local count=$(psql -U "$DB_USER" -h "$DB_HOST" -d "$target_db" \
        -t -c "SELECT COUNT(*) FROM \"$table_name\";" 2>/dev/null | tr -d ' ')

    log "âœ“ Table '$table_name' restored (${count} rows)"
}

download_from_r2() {
    local database=$1
    local date=$2

    log "Downloading ${database}_${date}* from R2..."
    rclone copy "r2:cs-backups/postgres/daily/${database}_${date}"* "$BACKUP_DIR" --progress
    log "âœ“ Download completed"
}

# Main
[[ $# -lt 1 ]] && usage

case "$1" in
    list)         list_backups "${2:-}" ;;
    verify)       [[ $# -lt 2 ]] && usage; verify_backup "$2" ;;
    restore)      [[ $# -lt 3 ]] && usage; restore_database "$2" "$3" ;;
    restore-table) [[ $# -lt 4 ]] && usage; restore_table "$2" "$3" "$4" ;;
    download)     [[ $# -lt 3 ]] && usage; download_from_r2 "$2" "$3" ;;
    *)            usage ;;
esac
```

#### 2.4 å¥åº·æ£€æŸ¥è„šæœ¬

åˆ›å»º `/backup/scripts/health-check.sh`ï¼š

```bash
#!/bin/bash
#
# Backup Health Check Script
#

set -euo pipefail

BACKUP_DIR="/backup/postgres/daily"
WEEKLY_DIR="/backup/postgres/weekly"
LOG_FILE="/backup/logs/health-check.log"

MAX_AGE_HOURS=25
MIN_SIZE_KB=100
DATABASES=("medusa_production" "strapi_production")

log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

send_alert() {
    local message=$1
    local level=${2:-WARNING}

    if [[ -n "${DISCORD_WEBHOOK_URL:-}" ]]; then
        local emoji="âš ï¸"
        [[ "$level" == "CRITICAL" ]] && emoji="ğŸš¨"
        curl -s -X POST "$DISCORD_WEBHOOK_URL" \
            -H "Content-Type: application/json" \
            -d "{\"content\": \"${emoji} **Backup ${level}**: ${message}\"}" \
            > /dev/null 2>&1 || true
    fi
    log "${level}: ${message}"
}

check_backup() {
    local db=$1
    local latest=$(ls -t "${BACKUP_DIR}/${db}_"*.dump 2>/dev/null | head -1)

    if [[ -z "$latest" ]]; then
        send_alert "No backup found for ${db}" "CRITICAL"
        return 1
    fi

    # Check age
    local age_hours=$(( ($(date +%s) - $(stat -c %Y "$latest")) / 3600 ))
    if [[ $age_hours -gt $MAX_AGE_HOURS ]]; then
        send_alert "${db}: Backup is ${age_hours}h old" "CRITICAL"
        return 1
    fi

    # Check size
    local size_kb=$(($(stat -c %s "$latest") / 1024))
    if [[ $size_kb -lt $MIN_SIZE_KB ]]; then
        send_alert "${db}: Backup too small (${size_kb}KB)" "WARNING"
        return 1
    fi

    # Check integrity
    if ! pg_restore --list "$latest" > /dev/null 2>&1; then
        send_alert "${db}: Backup corrupted!" "CRITICAL"
        return 1
    fi

    log "OK: ${db} - ${age_hours}h ago, ${size_kb}KB"
    return 0
}

generate_report() {
    echo ""
    echo "=============================================="
    echo "Backup Health Report - $(date '+%Y-%m-%d %H:%M:%S')"
    echo "=============================================="
    echo ""
    echo "Daily Backups:"
    for db in "${DATABASES[@]}"; do
        local latest=$(ls -t "${BACKUP_DIR}/${db}_"*.dump 2>/dev/null | head -1)
        if [[ -n "$latest" ]]; then
            local size=$(du -h "$latest" | cut -f1)
            local age=$(( ($(date +%s) - $(stat -c %Y "$latest")) / 3600 ))
            echo "  ${db}: ${size}, ${age}h ago"
        else
            echo "  ${db}: NO BACKUP!"
        fi
    done
    echo ""
    echo "Disk Usage:"
    df -h /backup | tail -1 | awk '{print "  Used: "$3" / "$2" ("$5")"}'
    echo ""
}

main() {
    log "=========================================="
    log "Health Check Started"
    log "=========================================="

    local failed=0

    # Check disk space
    local usage=$(df /backup | tail -1 | awk '{print $5}' | tr -d '%')
    if [[ $usage -gt 90 ]]; then
        send_alert "Disk usage critical: ${usage}%" "CRITICAL"
        failed=1
    fi

    # Check each database
    for db in "${DATABASES[@]}"; do
        check_backup "$db" || failed=1
    done

    generate_report

    log "=========================================="
    [[ $failed -eq 0 ]] && log "Health Check: ALL OK" || log "Health Check: ISSUES DETECTED"
    log "=========================================="

    return $failed
}

main "$@"
```

#### 2.5 è®¾ç½®è„šæœ¬æƒé™

```bash
chmod +x /backup/scripts/*.sh
```

### 3. é…ç½® rclone è¿æ¥ R2

#### 3.1 å®‰è£… rclone

```bash
curl -s https://rclone.org/install.sh | sudo bash
```

#### 3.2 åˆ›å»º rclone é…ç½®

```bash
mkdir -p ~/.config/rclone

cat > ~/.config/rclone/rclone.conf << 'EOF'
[r2]
type = s3
provider = Cloudflare
access_key_id = YOUR_R2_ACCESS_KEY_ID
secret_access_key = YOUR_R2_SECRET_ACCESS_KEY
endpoint = https://YOUR_ACCOUNT_ID.r2.cloudflarestorage.com
acl = private
EOF

chmod 600 ~/.config/rclone/rclone.conf
```

**è·å– R2 å‡­æ®ï¼š**
1. ç™»å½• Cloudflare Dashboard
2. è¿›å…¥ R2 â†’ Overview â†’ Manage R2 API Tokens
3. åˆ›å»º API Tokenï¼Œé€‰æ‹© "Object Read & Write" æƒé™

#### 3.3 éªŒè¯è¿æ¥å¹¶åˆ›å»º bucket

```bash
# æµ‹è¯•è¿æ¥
rclone lsd r2:

# åˆ›å»ºå¤‡ä»½ bucket
rclone mkdir r2:cs-backups
```

### 4. é…ç½® PostgreSQL è®¤è¯

åˆ›å»º `.pgpass` æ–‡ä»¶å®ç°å…å¯†ç è¿æ¥ï¼š

```bash
cat > ~/.pgpass << 'EOF'
localhost:5432:*:YOUR_DB_USER:YOUR_DB_PASSWORD
EOF

chmod 600 ~/.pgpass
```

**æµ‹è¯•è¿æ¥ï¼š**

```bash
psql -U cs -h localhost -d medusa_production -c "SELECT 1;"
```

### 5. è®¾ç½® Cron å®šæ—¶ä»»åŠ¡

#### 5.1 åˆ›å»º cron é…ç½®æ–‡ä»¶

```bash
cat > /backup/scripts/postgres-backup.cron << 'EOF'
# PostgreSQL Backup Cron Jobs
SHELL=/bin/bash
PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin
HOME=/home/coderisedev

# Daily logical backup - 00:00
0 0 * * * coderisedev /backup/scripts/backup-daily.sh >> /backup/logs/cron.log 2>&1

# Weekly physical backup - Sunday 02:00
0 2 * * 0 coderisedev /backup/scripts/backup-weekly.sh >> /backup/logs/cron.log 2>&1

# Health check - every 6 hours
0 */6 * * * coderisedev /backup/scripts/health-check.sh >> /backup/logs/cron.log 2>&1

# Log rotation - daily 03:00
0 3 * * * coderisedev find /backup/logs -name "*.log" -mtime +30 -delete
EOF
```

#### 5.2 å®‰è£… cron ä»»åŠ¡

```bash
sudo cp /backup/scripts/postgres-backup.cron /etc/cron.d/postgres-backup
sudo chmod 644 /etc/cron.d/postgres-backup
```

#### 5.3 éªŒè¯ cron å·²åŠ è½½

```bash
sudo systemctl restart cron
grep -r "backup" /etc/cron.d/
```

## ä½¿ç”¨æŒ‡å—

### æ‰‹åŠ¨æ‰§è¡Œå¤‡ä»½

```bash
# æ‰§è¡Œæ¯æ—¥å¤‡ä»½
/backup/scripts/backup-daily.sh

# æ‰§è¡Œæ¯å‘¨å¤‡ä»½
/backup/scripts/backup-weekly.sh
```

### æŸ¥çœ‹å¤‡ä»½

```bash
# åˆ—å‡ºæ‰€æœ‰å¤‡ä»½
/backup/scripts/restore.sh list

# åˆ—å‡ºç‰¹å®šæ•°æ®åº“çš„å¤‡ä»½
/backup/scripts/restore.sh list medusa_production

# æŸ¥çœ‹ R2 ä¸Šçš„å¤‡ä»½
rclone ls r2:cs-backups/postgres/daily/
```

### éªŒè¯å¤‡ä»½

```bash
/backup/scripts/restore.sh verify /backup/postgres/daily/medusa_production_20240115_000000.dump
```

### æ¢å¤æ•°æ®åº“

```mermaid
flowchart TD
    A[éœ€è¦æ¢å¤] --> B{æ¢å¤èŒƒå›´?}
    B -->|æ•´ä¸ªæ•°æ®åº“| C[restore å‘½ä»¤]
    B -->|å•ä¸ªè¡¨| D[restore-table å‘½ä»¤]
    B -->|ä» R2 ä¸‹è½½| E[download å‘½ä»¤]

    C --> C1["restore.sh restore backup.dump target_db"]
    D --> D1["restore.sh restore-table backup.dump db table"]
    E --> E1["restore.sh download db_name 20240115"]
```

```bash
# æ¢å¤åˆ°æ–°æ•°æ®åº“ï¼ˆæ¨èï¼Œä¸å½±å“ç”Ÿäº§ï¼‰
/backup/scripts/restore.sh restore \
    /backup/postgres/daily/medusa_production_20240115_000000.dump \
    medusa_restore

# æ¢å¤å•ä¸ªè¡¨åˆ°ç°æœ‰æ•°æ®åº“
/backup/scripts/restore.sh restore-table \
    /backup/postgres/daily/medusa_production_20240115_000000.dump \
    medusa_production \
    product

# ä» R2 ä¸‹è½½å¤‡ä»½
/backup/scripts/restore.sh download medusa_production 20240115
```

### å¥åº·æ£€æŸ¥

```bash
# æ‰‹åŠ¨è¿è¡Œå¥åº·æ£€æŸ¥
/backup/scripts/health-check.sh

# æŸ¥çœ‹æ—¥å¿—
tail -f /backup/logs/health-check.log
```

### é…ç½®å‘Šè­¦ï¼ˆå¯é€‰ï¼‰

è®¾ç½® Discord Webhook æ¥æ”¶å‘Šè­¦ï¼š

```bash
# æ·»åŠ åˆ° /etc/environment æˆ– ~/.bashrc
export DISCORD_WEBHOOK_URL="https://discord.com/api/webhooks/xxx/yyy"
```

## æ•…éšœæ’é™¤

### å¤‡ä»½å¤±è´¥ï¼špg_dump è¿æ¥é”™è¯¯

```bash
# æ£€æŸ¥ .pgpass æ–‡ä»¶æƒé™
ls -la ~/.pgpass  # åº”è¯¥æ˜¯ 600

# æµ‹è¯•æ•°æ®åº“è¿æ¥
psql -U cs -h localhost -d medusa_production -c "SELECT 1;"

# æ£€æŸ¥ PostgreSQL æ—¥å¿—
sudo tail -50 /var/log/postgresql/postgresql-*-main.log
```

### R2 åŒæ­¥å¤±è´¥

```bash
# æµ‹è¯• rclone é…ç½®
rclone lsd r2:

# æ£€æŸ¥é…ç½®æ–‡ä»¶
cat ~/.config/rclone/rclone.conf

# æ‰‹åŠ¨æµ‹è¯•åŒæ­¥
rclone sync /backup/postgres/daily r2:cs-backups/postgres/daily --dry-run
```

### Cron ä»»åŠ¡æœªæ‰§è¡Œ

```bash
# æ£€æŸ¥ cron æœåŠ¡
sudo systemctl status cron

# æŸ¥çœ‹ cron æ—¥å¿—
grep CRON /var/log/syslog | tail -20

# æ£€æŸ¥ cron æ–‡ä»¶è¯­æ³•
cat /etc/cron.d/postgres-backup
```

### ç£ç›˜ç©ºé—´ä¸è¶³

```bash
# æ£€æŸ¥ç£ç›˜ä½¿ç”¨
df -h /backup

# æ‰‹åŠ¨æ¸…ç†æ—§å¤‡ä»½
find /backup/postgres/daily -name "*.dump" -mtime +7 -delete

# è°ƒæ•´ä¿ç•™ç­–ç•¥ï¼ˆä¿®æ”¹è„šæœ¬ä¸­çš„ RETENTION_DAYSï¼‰
```

## æ–‡ä»¶æ¸…å•

```
/backup/
â”œâ”€â”€ postgres/
â”‚   â”œâ”€â”€ daily/              # æ¯æ—¥é€»è¾‘å¤‡ä»½
â”‚   â”‚   â”œâ”€â”€ medusa_production_YYYYMMDD_HHMMSS.dump
â”‚   â”‚   â””â”€â”€ strapi_production_YYYYMMDD_HHMMSS.dump
â”‚   â”œâ”€â”€ weekly/             # æ¯å‘¨ç‰©ç†å¤‡ä»½
â”‚   â”‚   â””â”€â”€ base_YYYYMMDD_HHMMSS.tar.gz
â”‚   â””â”€â”€ wal-archive/        # WAL å½’æ¡£ï¼ˆå¦‚å¯ç”¨ï¼‰
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ backup-daily.sh     # æ¯æ—¥å¤‡ä»½è„šæœ¬
â”‚   â”œâ”€â”€ backup-weekly.sh    # æ¯å‘¨å¤‡ä»½è„šæœ¬
â”‚   â”œâ”€â”€ restore.sh          # æ¢å¤å·¥å…·
â”‚   â”œâ”€â”€ health-check.sh     # å¥åº·æ£€æŸ¥
â”‚   â””â”€â”€ postgres-backup.cron # Cron é…ç½®
â””â”€â”€ logs/
    â”œâ”€â”€ backup-daily.log
    â”œâ”€â”€ backup-weekly.log
    â”œâ”€â”€ health-check.log
    â””â”€â”€ cron.log

~/.config/rclone/rclone.conf  # R2 é…ç½®
~/.pgpass                      # PostgreSQL è®¤è¯
/etc/cron.d/postgres-backup    # ç³»ç»Ÿ Cron ä»»åŠ¡
```

## å¿«é€Ÿå‚è€ƒå¡

| æ“ä½œ | å‘½ä»¤ |
|------|------|
| æ‰‹åŠ¨å¤‡ä»½ | `/backup/scripts/backup-daily.sh` |
| åˆ—å‡ºå¤‡ä»½ | `/backup/scripts/restore.sh list` |
| éªŒè¯å¤‡ä»½ | `/backup/scripts/restore.sh verify <file>` |
| æ¢å¤æ•°æ®åº“ | `/backup/scripts/restore.sh restore <file> <db>` |
| æ¢å¤å•è¡¨ | `/backup/scripts/restore.sh restore-table <file> <db> <table>` |
| å¥åº·æ£€æŸ¥ | `/backup/scripts/health-check.sh` |
| æŸ¥çœ‹ R2 | `rclone ls r2:cs-backups/postgres/daily/` |
| æŸ¥çœ‹æ—¥å¿— | `tail -f /backup/logs/backup-daily.log` |
